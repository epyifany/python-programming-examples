{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Mon Sep 24 09:29:16 2018\n",
    "\n",
    "@author: mniemier\n",
    "\"\"\"\n",
    "\n",
    "###################################################################################################\n",
    "# Import relevant Keras items, numpy, matplotlib\n",
    "###################################################################################################\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "###################################################################################################\n",
    "# load (downloaded if needed) the MNIST dataset\n",
    "# Assign training data and labels + tesing data and labels to X_train, y_train + X_test, y_test\n",
    "###################################################################################################\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "###################################################################################################\n",
    "# If you want to visualize MNIST images, uncomment this code...\n",
    "###################################################################################################\n",
    "# plt.subplot(221)\n",
    "# plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "# plt.subplot(222)\n",
    "# plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
    "# plt.subplot(223)\n",
    "# plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
    "# plt.subplot(224)\n",
    "# plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
    "# plt.show()\n",
    "\n",
    "###################################################################################################\n",
    "# Calculate the number of pixels in each image -- i.e., the image height * the image width\n",
    "#\n",
    "# Because we are working with a multi-layer perceptron network (i.e., a single layer of input\n",
    "# neurons), we want to create a single, 1D vector out of each image.\n",
    "# --> The .reshape function does this; note that X_train.shape[0] is simply the # of images in\n",
    "#     the train and test set (in this example, 60000)\n",
    "# --> Each training / testing image is reshaped to a 784 x 1 pixel vector\n",
    "###################################################################################################\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
    "\n",
    "###################################################################################################\n",
    "# Input pixel values are normalized from 0-255 to 0-1 -- this is common...\n",
    "###################################################################################################\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "###################################################################################################\n",
    "# Here, we create a 1 hot encoding of our labels -- i.e., the to_categorial command\n",
    "# converts a class vector (integers) to binary class matrix.\n",
    "#\n",
    "# We also record the nubmer of classes\n",
    "###################################################################################################\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "\n",
    "###################################################################################################\n",
    "# This code is used to define a network model\n",
    "# The simplest type of model is a sequential model\n",
    "#\n",
    "# The model needs to know what input shape it should expect. For this reason, the first layer \n",
    "# in a Sequential model (and only the first, because following layers can do automatic shape \n",
    "# inference) needs to receive information about its input shape. \n",
    "#\n",
    "# See more at:  https://keras.io/getting-started/sequential-model-guide/\n",
    "###################################################################################################\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=784, kernel_initializer='normal', activation='softmax'))\n",
    "\n",
    "###################################################################################################\n",
    "# This code is used to summarize a network model (see console output)\n",
    "###################################################################################################\n",
    "model.summary()\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# Before training a model, you need to configure the learning process, which is done via the \n",
    "# compile method. It receives three arguments:\n",
    "#  --> An Optimizer\n",
    "#  --> A loss function\n",
    "#  --> A list of metrics\n",
    "#\n",
    "#  These will be given to you, but the optimizer is almost always cross_entropy, and for a \n",
    "#  classification problem, the metric is accuracy...  Note that if you only have two classes, you\n",
    "#  might employ \"binary_crossentroy\", while if there are more than 2 classes, you can use \n",
    "#  \"categorical_crossentropy\" -- again, the training detail is abstracted away here...\n",
    "#\n",
    "# See more at: https://keras.io/getting-started/sequential-model-guide/#compilation\n",
    "###################################################################################################\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "###################################################################################################\n",
    "# This is the code that actually trains the network\n",
    "# Note that inputs are \n",
    "# --> a Numpy array of training data (and the labels)\n",
    "# --> a tuple of validation data\n",
    "# --> the # of epochs in which training occurs (in each epoch, we iterate over all training data)\n",
    "# --> the batch size -- i.e., the number of samples tested before a gradient update\n",
    "#       --> if the number if small, you do many weight updates during training, so your weights\n",
    "#           may be better...\n",
    "#       --> however, training run time also increases as you do more weight updates\n",
    "#       --> a batch size of around 100 should be a happy medium\n",
    "# --> verbose=1 shows more details about the progess of training; verbose=2 shows less detail\n",
    "#       --> both versions show training (acc) and testing (val_acc) accuracy during training\n",
    "###################################################################################################\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=100, verbose=2)\n",
    "\n",
    "###################################################################################################\n",
    "# Print out the final network accuracy with test data\n",
    "###################################################################################################\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# To visualize the learned filters for MNIST, use the code below\n",
    "# Note that the first line will get the weight values for the first (and only) layer of the network\n",
    "#   --> first_layer_weigths is a 784, 10 tuple (i.e., 784 weights for 10 classes)\n",
    "#   --> I simply get all of the weights for a given image class (i.e., test_number = 0) will\n",
    "#       gather the weights for the 0 class and then illustrate them with a color map\n",
    "###################################################################################################\n",
    "\n",
    "first_layer_weights = model.layers[0].get_weights()[0]\n",
    "\n",
    "test_number = 0\n",
    "\n",
    "image_array = [0] * len(first_layer_weights)\n",
    "for j in range(len(first_layer_weights)):\n",
    "   image_array[j] = first_layer_weights[j][test_number]\n",
    "   \n",
    "\n",
    "plt.imshow(np.array(image_array).reshape(28,28), cmap='bwr')\n",
    "plt.colorbar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
